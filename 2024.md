# 2024
* **NavigatiOn with goal MAsked Diffusion** (**NoMaD**)
  * title and link: [NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration](https://arxiv.org/abs/2310.07896)
  * information: ICRA 2024 best paper UCBerkeley (Sergey Levine)
  * problem and position: single network for goal-directed navigation and goal-agnostic exploration
  * method overview: Transformer encoder with optional goal condition masking for observed images and diffusion policy for future actions
  * results: 
    ![NoMaD_result1](assets/2024/NoMaD_result1.png)
    ![NoMaD_result2](assets/2024/NoMaD_result2.png)
  * method details: 
    * ViNT as the encoder backbone for goal-conditioned navigation
    * ViKiNGâ€™s topological graph for goal-free exploration
    * 50% probability goal masking during training
    * 1D conditional UNet as the diffusion policy
    * train on combination of GNM and SACSoN datasets
    ![NoMaD_method](assets/2024/NoMaD_method.png)

* **Robotics Transformer X** (**RT-X**)
  * title and link: [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864)
  * information: ICRA 2024 best paper 44 institutions
  * problem and position: union of open-source robotics datasets and attempt to general training
  * method overview: RT-1 and RT-2 on Open X-Embodiment dataset
  * teaser: 
    ![RT-X_teaser](assets/2024/RT-X_teaser.png)
  * results: 
    ![RT-X_result](assets/2024/RT-X_result.png)

* **Universal Simulator** (**UniSim**)
  * title and link: [Learning Interactive Real-World Simulators](https://arxiv.org/abs/2310.06114)
  * information: ICLR 2024 outstanding paper UCBerkeley (Pieter Abbeel)
  * problem and position: action-conditioned video prediction enables robot learning
  * method overview: accept language, motor action, camera motions as actions, then action-conditioned video diffusion model
  * teaser: 
    ![UniSim_teaser](assets/2024/UniSim_teaser.png)
  * results: used for high-level VLM policy and low-level RL policy training
  * method details: 
    * different video datasets cover different information
      ![UniSim_method](assets/2024/UniSim_method.png)
    * texts by T5 language embedding, motor actions, camera motions
    * video 3D UNet diffusion model predicts next frames conditioned on observed frames and actions autoregressively
    * action-condition by classifier-free guidance
    * 5.6B parameters
    * experiment PaLM-E image-goal conditioned VLM policy and PaLI VLA policy with learned reward function for block rearrangement on 10k generated videos
