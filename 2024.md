# 2024
* **Generative Image Dynamics** (**GID**)
  * title and link: [Generative Image Dynamics](https://arxiv.org/abs/2309.07906)
  * information: CVPR 2024 best paper Google
  * problem and position: model image-space motion for turning single image to looping video or interactive demo
  * method overview: diffusion model predicts Fourier domain spectral volumes, Fourier transform to warp motioned future frames
  * teaser: 
    ![GID_teaser](assets/2024/GID_teaser.png)
  * results: 
    ![GID_result1](assets/2024/GID_result1.png)
    ![GID_result2](assets/2024/GID_result2.png)
  * method details: 
    * motion texture as 2D displacement maps $\{F_t \mid I_t(\mathbf{p} + F_t(\mathbf{p})) = I_0(\mathbf{p}), t = 1, \ldots, T\}$
    * directly predicting motion texture scales with $T$
    * Fast Fourier Transform to frequency domain $S(\mathbf{p}) = FFT(F(\mathbf{p}))$ and only low-freq $K = 16$ enough
    * latent diffusion model predicts $4K$-channel 2D motion spectrum map, with $4$ Fourier coefficients each frequency
      ![GID_method1](assets/2024/GID_method1.png)
    * normalization for stable training concentrates to low-freq, so normalize per frequency
      ![GID_method2](assets/2024/GID_method2.png)
      ![GID_method3](assets/2024/GID_method3.png)
    * directly outputting $4K$ channels yields over-smoothed results, so first train conditioning on frequency embedding to predict single $4$-channel coefficients, then freeze and insert attention layers for coordinating different frequencies and fine-tune
    * rendering with additional multi-scale ResNet-34 features soft-warped by $F_t = FFT^{-1}(\hat{S})$ and $W = \frac{1}{T} \sum_t \|F_t\|_2$ to decode
      ![GID_method4](assets/2024/GID_method4.png)
    * train from scratch for 6 days with 16 A100s
    * collect 3015 videos as >150k image-motion pairs

* **Rich Automatic Human Feedback** (**RAHF**)
  * title and link: [Rich Human Feedback for Text-to-Image Generation](https://arxiv.org/abs/2312.10240)
  * information: CVPR 2024 best paper Google
  * problem and position: fine-grained human feedback on text-image alignment for text2image generation
  * method overview: fine-grained annotated text-image alignment dataset and transformer model to predict the human feedbacks
  * results: can help text2image generation models
  * method details: 
    * RichHF-18K dataset from Pick-a-Pic
    * marking image regions, marking text words, annotating rate
      ![RAHF_method1](assets/2024/RAHF_method1.png)
    * multimodal transformer with multiple heads
      ![RAHF_method2](assets/2024/RAHF_method2.png)
    * finetune Muse on the self-generated images with high RAHF-predicted score
    * Muse inpainting with RAHF-predicted implausibility heatmap

* **NavigatiOn with goal MAsked Diffusion** (**NoMaD**)
  * title and link: [NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration](https://arxiv.org/abs/2310.07896)
  * information: ICRA 2024 best paper UCBerkeley (Sergey Levine)
  * problem and position: single network for goal-directed navigation and goal-agnostic exploration
  * method overview: Transformer encoder with optional goal condition masking for observed images and diffusion policy for future actions
  * results: 
    ![NoMaD_result1](assets/2024/NoMaD_result1.png)
    ![NoMaD_result2](assets/2024/NoMaD_result2.png)
  * method details: 
    * ViNT as the encoder backbone for goal-conditioned navigation
    * ViKiNGâ€™s topological graph for goal-free exploration
    * 50% probability goal masking during training
    * 1D conditional UNet as the diffusion policy
    * train on combination of GNM and SACSoN datasets
    ![NoMaD_method](assets/2024/NoMaD_method.png)

* **Robotics Transformer X** (**RT-X**)
  * title and link: [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864)
  * information: ICRA 2024 best paper 44 institutions
  * problem and position: union of open-source robotics datasets and attempt to general training
  * method overview: RT-1 and RT-2 on Open X-Embodiment dataset
  * teaser: 
    ![RT-X_teaser](assets/2024/RT-X_teaser.png)
  * results: 
    ![RT-X_result](assets/2024/RT-X_result.png)

* **Universal Simulator** (**UniSim**)
  * title and link: [Learning Interactive Real-World Simulators](https://arxiv.org/abs/2310.06114)
  * information: ICLR 2024 outstanding paper UCBerkeley (Pieter Abbeel)
  * problem and position: action-conditioned video prediction enables robot learning
  * method overview: accept language, motor action, camera motions as actions, then action-conditioned video diffusion model
  * teaser: 
    ![UniSim_teaser](assets/2024/UniSim_teaser.png)
  * results: used for high-level VLM policy and low-level RL policy training
  * method details: 
    * different video datasets cover different information
      ![UniSim_method](assets/2024/UniSim_method.png)
    * texts by T5 language embedding, motor actions, camera motions
    * video 3D UNet diffusion model predicts next frames conditioned on observed frames and actions autoregressively
    * action-condition by classifier-free guidance
    * 5.6B parameters
    * experiment PaLM-E image-goal conditioned VLM policy and PaLI VLA policy with learned reward function for block rearrangement on 10k generated videos
